<!DOCTYPE html>
<!-- saved from url=(0038)https://zhuanlan.zhihu.com/p/337649487 -->
<html lang="zh" data-hairline="true" data-theme="light" data-react-helmet="data-theme"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>End-to-End Object Detection with Transformers论文详解 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="keywords" content="目标检测,计算机视觉"><meta data-react-helmet="true" name="description" content="FAIR对DETR简单介绍https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers首先是一些总括，本文方法在文章中部。 1.舍弃一些手工部件，不依赖NMS和先验框，或者是Anchor的设定,简化了检测…"><meta data-react-helmet="true" property="og:title" content="End-to-End Object Detection with Transformers论文详解"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/337649487"><meta data-react-helmet="true" property="og:description" content="FAIR对DETR简单介绍https://ai.facebook.com/blog/end-to-end-object-detection-with-transformers首先是一些总括，本文方法在文章中部。 1.舍弃一些手工部件，不依赖NMS和先验框，或者是Anchor的设定,简化了检测…"><meta data-react-helmet="true" property="og:image" content="https://pic4.zhimg.com/v2-99e7ec86dddff81164501e45b4590790_720w.jpg?source=172ae18b"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.a53ae37b.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.a53ae37b.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.bbce8f18.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.cbade8f9.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.8f6c52aa.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/heifetz/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/heifetz/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.app.216a26f4.7113513fb3a612c90465.css" rel="stylesheet"><script defer="" crossorigin="anonymous" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/init.js.下載" data-sentry-config="{&quot;dsn&quot;:&quot;https://2d8d764432cc4f6fb3bc78ab9528299d@crash2.zhihu.com/1224&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;1695-fb2eeeec&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><style data-emotion-css="1cd9gw4">.css-1cd9gw4{margin-left:.3em;}</style><style data-emotion-css="qbubgm">.css-qbubgm{margin-left:0;}</style><script charset="utf-8" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.zswsdid.84dde64f1825d6da8d77.js.下載"></script><link rel="stylesheet" type="text/css" href="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.user-hover-card.216a26f4.a21a39cedb849406303d.css"><script charset="utf-8" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.user-hover-card.224c2f9be7103be1e6bc.js.下載"></script><link rel="stylesheet" type="text/css" href="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.Labels.216a26f4.7d19d2afdc588e36471f.css"><script charset="utf-8" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.Labels.48c279d5e2cc40efda57.js.下載"></script><link rel="stylesheet" type="text/css" href="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.modals.216a26f4.6b0a511460867b1595f8.css"><script charset="utf-8" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.modals.8c862ae112560ca79e04.js.下載"></script><link rel="stylesheet" type="text/css" href="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.comments-modals.216a26f4.39d2b57c6fd2cae24672.css"><script charset="utf-8" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.comments-modals.786d13c2b88e925b0e26.js.下載"></script><style data-emotion="css"></style><link rel="stylesheet" type="text/css" href="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.richinput.216a26f4.d7eb544a00018fff1eb6.css"><script charset="utf-8" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.richinput.3b38363440b34815974f.js.下載"></script></head><body class="WhiteBg-body" data-react-helmet="class"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;&quot;}" data-zop="{&quot;authorName&quot;:&quot;青山不改&quot;,&quot;itemId&quot;:337649487,&quot;title&quot;:&quot;End-to-End Object Detection with Transformers论文详解&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;337649487&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1268.67px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a class="ZhihuLogoLink" href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 64 30" fill="#0066FF" width="64" height="30"><path d="M29.05 4.582H16.733V25.94h3.018l.403 2.572 4.081-2.572h4.815V4.582zm-5.207 18.69l-2.396 1.509-.235-1.508h-1.724V7.233h6.78v16.04h-2.425zM14.46 14.191H9.982c0-.471.033-.954.039-1.458v-5.5h5.106V5.935a1.352 1.352 0 0 0-.404-.957 1.378 1.378 0 0 0-.968-.396H5.783c.028-.088.056-.177.084-.255.274-.82 1.153-3.326 1.153-3.326a4.262 4.262 0 0 0-2.413.698c-.57.4-.912.682-1.371 1.946-.532 1.453-.997 2.856-1.31 3.693C1.444 8.674.28 11.025.28 11.025a5.85 5.85 0 0 0 2.52-.61c1.119-.593 1.679-1.502 2.054-2.883l.09-.3h2.334v5.5c0 .5-.045.982-.073 1.46h-4.12c-.71 0-1.39.278-1.893.775a2.638 2.638 0 0 0-.783 1.874h6.527a17.717 17.717 0 0 1-.778 3.649 16.796 16.796 0 0 1-3.012 5.273A33.104 33.104 0 0 1 0 28.74s3.13 1.175 5.425-.954c1.388-1.292 2.631-3.814 3.23-5.727a28.09 28.09 0 0 0 1.12-5.229h5.967v-1.37a1.254 1.254 0 0 0-.373-.899 1.279 1.279 0 0 0-.909-.37z"></path><path d="M11.27 19.675l-2.312 1.491 5.038 7.458a6.905 6.905 0 0 0 .672-2.218 3.15 3.15 0 0 0-.28-2.168l-3.118-4.563zM51.449 15.195V5.842c4.181-.205 7.988-.405 9.438-.483l.851-.05c.387-.399.885-2.395.689-3.021-.073-.25-.213-.666-.638-.555a33.279 33.279 0 0 1-4.277.727c-2.766.321-3.97.404-7.804.682-6.718.487-12.709.72-12.709.72a2.518 2.518 0 0 0 .788 1.834 2.567 2.567 0 0 0 1.883.706c2.278-.095 5.598-.25 8.996-.41v9.203h-12.78c0 .703.281 1.377.783 1.874a2.69 2.69 0 0 0 1.892.777h10.105v7.075c0 .887-.464 1.192-1.231 1.214h-3.92a4.15 4.15 0 0 0 .837 1.544 4.2 4.2 0 0 0 1.403 1.067 6.215 6.215 0 0 0 2.71.277c1.36-.066 2.967-.826 2.967-3.57v-7.607h11.28c.342 0 .67-.135.91-.374.242-.239.378-.563.378-.902v-1.375H51.449z"></path><path d="M42.614 8.873a2.304 2.304 0 0 0-1.508-.926 2.334 2.334 0 0 0-1.727.405l-.376.272 4.255 5.85 2.24-1.62-2.884-3.98zM57.35 8.68l-3.125 4.097 2.24 1.663 4.517-5.927-.375-.277a2.32 2.32 0 0 0-1.722-.452 2.327 2.327 0 0 0-1.536.896z"></path></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://www.zhihu.com/column/c_1312325735072657408">计算机视觉</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button></div></div></div><div class="Sticky--holder" style="position: relative; inset: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><img class="TitleImage" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-99e7ec86dddff81164501e45b4590790_1440w.jpg" alt="End-to-End Object Detection with Transformers论文详解"><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">End-to-End Object Detection with Transformers论文详解</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="青山不改"><meta itemprop="image" content="https://pic4.zhimg.com/v2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b"><meta itemprop="url" content="https://www.zhihu.com/people/xieqingtao"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xieqingtao"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-8d2597784f4f6e279e3b5d0afa4c83d7_xs.jpg" srcset="https://pic4.zhimg.com/v2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b 2x" alt="青山不改"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xieqingtao">青山不改</a></div></div></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="ztext AuthorInfo-badgeText">在CV苦海挣扎的研究僧</div></div></div></div></div></div><div class="LabelContainer-wrapper"></div><div><span class="Voters"><button type="button" class="Button Button--plain">8 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><blockquote>FAIR对DETR简单介绍<a href="https://link.zhihu.com/?target=https%3A//ai.facebook.com/blog/end-to-end-object-detection-with-transformers" class=" external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043"><span class="invisible">https://</span><span class="visible">ai.facebook.com/blog/en</span><span class="invisible">d-to-end-object-detection-with-transformers</span><span class="ellipsis"></span></a></blockquote><p>首先是一些总括，本文方法在文章中部。</p><h2>1.舍弃一些手工部件，不依赖NMS和先验框，或者是Anchor的设定,简化了检测的pipeline</h2><h2>2.DETR架构包括两部分：<b>基于集合的全局损失与Transfomer encoder-decoder</b>。</h2><p>其中DETR framework包含一基于<b>集合的全局损失</b>，通过<b>二分图匹配</b>来强制<b>唯一的预测</b>，以前也有RNN对目标检测的尝试，但是速度太慢，无法并行计算。<b>Transfomer可并行计算。</b></p><h2>3.  DETR的工作基于这两个方面：<b>set predication、Transformer</b></h2><h3>3.1     集合预测的二分图匹配 <b>bipartite matching loss for set prediction</b></h3><p>传统方法为了避免相近框的重叠问题，选择使用后处理的方法，如非极大 值抑制，</p><p>但是<b>set prediction是无后处理的</b>。他们需要对所有预测元素之间的交互进行建模的全局推理方案，以避免冗余。</p><p>对于恒定的集合预测，可以使用<b>密集的全连接层</b>，有效但是<b>耗费太多算力</b>,对于set prediction 这种问题一般的方法使用'auto-regressive sequence models'，例如RNN。</p><p>损失函数在经过prediction的置换之后应该不变，通用的解决方法是设计一个基于<b>匈牙利算法</b>的损失函数，找到GT与预测的最优匹配。</p><p class="ztext-empty-paragraph"><br></p><p><b>匈牙利算法</b>就是一个最优匹配问题，从矩阵角度来说，就是每一行抽出最小值，然后让该行所有值都去减掉抽出的最小值，会至少获得一个0，然后剩余的所有行也做这样的操作。</p><p>第一次全部操作完后，如果每一个行都有一个不同列的0值，那么此时就是各个0值位置最佳的匹配。</p><p>如果0的行列都不相同，则不必再计算，否则，继续上述操作，直到0值的行列均不相同。</p><h3>3.2  <b>encoder-decoder architectures based on the Transfomer</b></h3><p>基于注意力机制模型的一个主要优点是他能够进行全局的并行计算，和优越的记忆能力。</p><p>因此在long sequences上比RNN更合适。</p><h2>4. OD相关工作。</h2><p><b>Two-stage predict boxs w.r.t. proposals</b></p><p><b>one-stage predict anchor or a grid of possible object centers</b></p><p><b>Anchor-base 与 anchor-free</b>  Bridging the Gap Between Anchor-based and Anchor-free</p><p>Detection via Adaptive Training Sample Selection 研究发现两者区别在于正负样本的选取方法不同。<b>论文提出ATSS</b>。**本文则与两种方法都不同，舍弃这种预先设置，直接用absolute box预测输入图片，而非预测anchor。</p><p class="ztext-empty-paragraph"><br></p><p><b>set-based loss</b> 早期采用二分匹配损失，如SSD等算法(见论文书签object detection)，然而不同预测之间的关系用CNN或这全连接层来表示，并用手工设计的后处理来提高性能。<br>而最近的一些算法，如Faster RCNN，focal loss，在GT与预测间用非唯一分配的规则，并且用上NMS。</p><p><b>learnable NMS</b>可学习的NMS等方法，对不同预测间的关系用attention进行建模。使用direct set损失，它们不需要任何后处理步骤。但是，这些方法采用了其他手工制作的上下文特征（例如提案框坐标）来有效地对检测之间的关系进行建模，而我们正在寻找能够减少模型中编码的先验知识的解决方案。</p><h2>5.模型可以查看图像的其他区域，以帮助对边界框中的对象作出决定。</h2><p>它还可以<b>根据图像中对象之间的关系或相关性进行预测</b>。 例如，如果DETR预测图像包含一个站在海滩上的人，它就知道部分被遮挡的物体更有可能是冲浪板。 相反，其他检测模型只是机械的地预测每个对象。</p><h2>6.促进视觉与NLP领域的结合，对以后<b>同时处理视觉与语言信息</b>提供一定的思路</h2><hr><p class="ztext-empty-paragraph"><br></p><h2>本文方法 The DETR model(位于论文第三节)</h2><h2>3.1.set prediction loss</h2><p>一次的预测N个，N大于一张图片中的目标个数，这里的一个主要困难是如何对GT与预测之间的类别，位置，</p><p>大小进行评分比对。损失函数会产生一个最佳的二分匹配，然后对包围框进行特定优化。<br> 关于二分图匹配，其实就是一个最优匹配问题。</p><p><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation" alt="[公式]" eeimg="1" data-formula="\hat{\sigma}= argmin\sum_{i}^{N}(L_{match}(y_{i},\hat{y}_{\sigma (i)})                （1）"></p><p>  上式中，N表示一次预测的个数，大于图片中的目标个数，y表示预测值，并且y也是N个，不足的用空集（NO-object）来pading。y_hat 表示预测的N个。为了匹配这两个集合（y和y_hat），用上面这个式子来计算。</p><p><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="L_{match}(y_{i},\hat{y}_{\sigma (i)})"> 是成对匹配的cost，用匈牙利算法来计算。</p><p>  匹配cost考虑到了类别的损失，和边界框的损失。<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula="y_{i} = (c_{i},b_{i})">,其中<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="c_{i}"> 表示的是类别标签，可能为空集。<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula="b_{i} = [0,1]^{4 }"> 表示的是box的中心坐标，和高和宽相对于image size（感觉第四个是image size）。</p><p>对于 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(5)" alt="[公式]" eeimg="1" data-formula="\sigma_{i}"> 定义了  <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="c_{i}"> 其具体为 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(6)" alt="[公式]" eeimg="1" data-formula="\hat{p}_{\sigma(i)}(C_{i})"> ，<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(7)" alt="[公式]" eeimg="1" data-formula="\hat{b}_{\sigma (i)}"></p><p>预测box,<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="L_{match}(y_{i},\hat{y}_{\sigma (i)})"> 具体为</p><p><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(8)" alt="[公式]" eeimg="1" data-formula="-1_{\{C_{i}\neq\phi\}}\hat{P}_{\sigma(i)}(c_{i})+1_{\{C_{i}\neq\phi\}}L_{box}(b_{i},\hat{b}_{\sigma(i)})"></p><p>这种寻找匹配的过程与anchor和proposals与GT匹配的过程有相同的作用，最大的区别是找到一对一的匹配进行<b>无重复的直接 set predict</b>。</p><p><b>损失函数</b>，就是类别损失与box损失的线性组合:</p><p><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(9)" alt="[公式]" eeimg="1" data-formula="L_{Hungarian}(y,\hat{y}) =\sum_{i=1}^{N}[-\log\hat{P}_{\sigma(i)}(C_{i})+1_{\{C_{i}\neq\phi\}}L_{box}(b_{i},\hat{b}_{\sigma(i)})]                （2）"></p><p>（2）式中，<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(10)" alt="[公式]" eeimg="1" data-formula="\hat{\sigma}">是（1）中的最佳匹配，当<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula="c_{i}=\phi">时，把log-probability 权重降低10倍，来解决正负样本不平衡，这类似于Faster R-CNN中，训练过程为了平衡正负样本所采取的subsampling（二次抽样）。</p><p><b>后面说时在目标和<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(12)" alt="[公式]" eeimg="1" data-formula="\phi">之间的匹配cost不依赖于预测，这意味着这种情况下，cost是固定的，因此在匹配cost里，使用了 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(13)" alt="[公式]" eeimg="1" data-formula="\hat{P}_{\sigma(i)}(c_{i})">来代替log-probability .然后这就i让类预测能够通约到<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="L_{box(.,.)}"> 。</b>不同于其他方法那样有先验框去预测周边误差，我们的方法是直接预测框，这种方法虽然更直接，但是带来的问题是损失的相对尺度问题。直接使用L1损失对不同大小box会有不同尺度的问题，<b>为了缓解这个问题</b> 使用L1损失和广义IOU损失的线性组合，该尺度是不变的。（为啥不变呢？）</p><p><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(15)" alt="[公式]" eeimg="1" data-formula="L_{box}(b_{i},\hat{b}_{\sigma(i)})"> 具体表示为：<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(16)" alt="[公式]" eeimg="1" data-formula="\lambda_{iou}L_{iou}(b_{i},\hat{b}_{\sigma(i)})+\lambda_{L1}||b_{i}-\hat{b}_{\sigma(i)}||_{1}"></p><p> 其中 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(17)" alt="[公式]" eeimg="1" data-formula="\lambda_{iou},\lambda_{L1}"> 都属于R，都是超参数。</p><p>这两个损失函数都在batch中用目标数目来进行归一化。</p><h2>3.2 DETR的架构</h2><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-16dfa1fb8d1c6d8048caf760f92958f3_b.png" data-size="normal" class="content_image"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;" data-size="normal" class="content_image lazy" data-actualsrc="https://pic4.zhimg.com/v2-16dfa1fb8d1c6d8048caf760f92958f3_b.png"><figcaption>Image</figcaption></figure><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-10eabf880fe88dc59c9673952e6cb5fb_b.png" data-size="normal" class="content_image"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;" data-size="normal" class="content_image lazy" data-actualsrc="https://pic4.zhimg.com/v2-10eabf880fe88dc59c9673952e6cb5fb_b.png"><figcaption>Image</figcaption></figure><p class="ztext-empty-paragraph"><br></p><p>如图所示，DETR的架构很简单（官方简单。。）包含三个部分：<b>1.CNN主干来提取特征</b>，<b>2.encoder-decoder Transfomer,3.FFN，完成最后的检测预测。</b></p><p>实现相对简单，只要能提供CNN和Transfomer就能够在任何深度框架下用50行代码实现（pytorch）</p><h3>1.Backbone（骨干网络）</h3><p>使用CNN提取特征，把<img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(18)" alt="[公式]" eeimg="1" data-formula="3\times H\times W">的图片变成  <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(19)" alt="[公式]" eeimg="1" data-formula="2048\times \frac{H}{32}\times \frac{W}{32}">的特征图，这里要注意，对于同一batch的图片，要对其resize，<b>用0-padding方法，保证他们的(H,W)相同</b>。</p><h3>2.Transfomer encoder（具体其实可看论文Attention is all  you need）</h3><p class="ztext-empty-paragraph"><br></p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-d1f2e4794f41020128e56b3f10a9108f_b.png" data-size="normal" class="content_image"/></noscript><img src="data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;" data-size="normal" class="content_image lazy" data-actualsrc="https://pic4.zhimg.com/v2-d1f2e4794f41020128e56b3f10a9108f_b.png"><figcaption>Image</figcaption></figure><p><br>2.1 首先用 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(20)" alt="[公式]" eeimg="1" data-formula="1\times 1">的卷积讲原来C=2048维降到d维，就形成了一个 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(21)" alt="[公式]" eeimg="1" data-formula="d \times H\times W"> 的特征图，encoder的输入必须是序列，因此再变成 <img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula="d\times HW"> 的特征图，这里每一个encoder都有<b>multi-head self-attention和FFN</b>.因为transformer架构的原因，其缺少位置信息（<b>permutation-invariant</b>），还需为每个attention层加入位置信息（不是学到的，是手工的，学到的效果不好）</p><p>这里 <b>permutation-invariant</b>指的是特征之间没有空间位置关系，例如多层感知机，改变像素的位置对最后的结果没有影响，但是对于<b>卷积神经网络</b>而言，特征是有位置关系的。这里对于<b>Transfomer架构来说，是没有位置关系的</b>因此需要添加位置信息。</p><h3>3.Transfomer decoder</h3><p>Transfomer decoder 由<b>多头自注意力和encoder-decoder注意力机制</b>组成。与传统的Trans不同的地方在于对N个objects进行decoder时是<b>并行</b>的。</p><blockquote>这里解码时候，也是permutation-invariant的</blockquote><p>因此N个input特征必须是不同的来产生不同的结果，这些输入的特征是学到的位置编码信息，称之为<i><b>object queriers</b></i>.同样将这些位置编码输入到所有的attention层。解码器将这N个对象查询转换成embeding的输出。然后，它们被前馈网络(在下一小节中描述)独立解码成box坐标和类别标签，得到N个最终预测。利用self- and encoder-decoder attention，该模型利用所有对象之间的成对关系对所有对象进行全局推理，同时能够将整个图像用作上下文。</p><h3>4.Prediction feed-forward networks(FFNs)（预测前馈网络）</h3><p>最终的预测是由一个<b>3层感知器</b>和一个<b>线性投影层</b>计算的，感知器具有ReLU激活函数和隐藏维数d。FFN预测输入图像中盒子的归一化中心坐标、高度和宽度，线性层使用softmax函数预测类别标签。因为我们预测一组固定大小的n个边界框，其中n通常比图像中感兴趣对象的实际数量大得多，所以使用额外的特殊类标签∅来表示在一个槽内没有检测到对象。该类在标准对象检测方法中扮演着与“背景”类相似的角色。</p><blockquote>linear projection layer（<b>线性投影层</b>）：Projection layer是为了减少计算量，它的作用和全连接layer很像，就是对输出向量做一下压缩，从而能把高纬度的信息降维，减小cell unit的维度，从而减小相关参数矩阵的参数数目。  projection layer也是使用 1x1Conv 的网络结构，作用是希望把高维特征映射到低维空间去。</blockquote><h3>5.Auxiliary decoding losses(辅助解码损失).</h3><p>我们发现在训练期间在解码器中使用<i><b>auxiliary losses</b></i>是有帮助的，特别是有助于模型输出每个类的正确对象数。我们在每个解码器层后添加预测FFNs和匈牙利损失。所有预测FFNs共享它们的参数。我们使用一个额外的共享层范数来标准化来自不同解码器层的预测帧速率网络的输入。</p><blockquote>auxiliary losses：这种方法是  《 Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character-level language modeling with deeper self-attention. In: AAAI Conference on Artificial Intelligence (2019)》这篇论文提到的， 大概就是辅助损失有助于优化学习过程。</blockquote><h2>实验</h2><p>···后面补充</p></div></div><div class="ContentItem-time">编辑于 2020-12-17</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19596960&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19596960" target="_blank"><div class="Popover"><div id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content">目标检测</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19590195&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19590195" target="_blank"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content">计算机视觉</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 289.333px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;337649487&quot;}}}"><span><button aria-label="赞同 8 " type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 8</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><div class="css-qbubgm"><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>2 条评论</button></div><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Heart Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2 8.437C2 5.505 4.294 3.094 7.207 3 9.243 3 11.092 4.19 12 6c.823-1.758 2.649-3 4.651-3C19.545 3 22 5.507 22 8.432 22 16.24 13.842 21 12 21 10.158 21 2 16.24 2 8.437z" fill-rule="evenodd"></path></svg></span>喜欢</button><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="Post-SideActions" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 9">赞同 8</div></div></button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover14-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover14-content"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div><div class="Sticky--holder" style="position: static; inset: auto auto 0px 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://www.zhihu.com/column/c_1312325735072657408"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/4b70deef7_xs.jpg" srcset="https://pic1.zhimg.com/4b70deef7_l.jpg?source=172ae18b 2x" alt="计算机视觉"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://www.zhihu.com/column/c_1312325735072657408"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content">计算机视觉</div></div></a></h2><div class="ContentItem-meta">小样本的目标检测领域，及机器视觉和深度学习相关</div></div><div class="ContentItem-extra"><a href="https://www.zhihu.com/column/c_1312325735072657408" type="button" class="Button">进入专栏</a></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1269px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled="" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/149252931" class="PostItem"><div><h1 class="PostItem-Title">论文解读：End-to-End Object Detection with Transformers</h1><p class="PostItem-Summary">先记录几点：匈牙利算法： 看这里吧spatial positional encoding：应该也就是用余弦函数。 object query：全靠transformer自己学习。 摘要我们把目标检测看做是一种set prediction的问题，…</p><div class="PostItem-Footer"><span>布尔佛洛哥...</span><span class="PostItem-FooterTitle">发表于AI论文阅...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/336016003" class="PostItem"><div><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-c0bb9a6315e1f89ee17a8b7e57809cfc_250x0.jpg" srcset="https://pic1.zhimg.com/v2-c0bb9a6315e1f89ee17a8b7e57809cfc_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="OneNet: End-to-End One-Stage Object Detection"><h1 class="PostItem-Title">OneNet: End-to-End One-Stage Object Detection</h1><div class="PostItem-Footer"><span>孙培泽</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/228810324" class="PostItem"><div><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-f3c0d6da54c3334decadacf215a30124_250x0.jpg" srcset="https://pic1.zhimg.com/v2-f3c0d6da54c3334decadacf215a30124_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="VarifocalNet：IoU-aware 密集目标检测器（已开源）"><h1 class="PostItem-Title">VarifocalNet：IoU-aware 密集目标检测器（已开源）</h1><div class="PostItem-Footer"><span>CVer计...</span><span class="PostItem-FooterTitle">发表于CVer计...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/268842582" class="PostItem"><div><img src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-693d49892b0c86a8c45b3f6194e4f6ff_250x0.jpg" srcset="https://pic2.zhimg.com/v2-693d49892b0c86a8c45b3f6194e4f6ff_qhd.jpg?source=172ae18b 2x" class="PostItem-TitleImage" alt="【论文重读】Relation Networks for Object Detection"><h1 class="PostItem-Title">【论文重读】Relation Networks for Object Detection</h1><div class="PostItem-Footer"><span>贤鱼卓君</span><span class="PostItem-FooterTitle">发表于贤鱼从零开...</span></div></div></a><button class="PagingButton PagingButton-Next" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{}"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{}"><div class="CommentsV2 CommentsV2--withEditor CommentsV2-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">2 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div><div class="CommentsV2-footer CommentEditorV2--normal"><div class="CommentEditorV2-inputWrap"><div class="InputLike CommentEditorV2-input Editable"><div class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-3nui3" style="white-space: pre-wrap;">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-3nui3" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; overflow-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="3nui3" data-offset-key="dsihv-0-0"><div data-offset-key="dsihv-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="dsihv-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/webp,image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div><div class="CommentEditorV2-inputUpload"><div class="CommentEditorV2-popoverWrap"><div class="Popover CommentEditorV2-inputUpLoad-Icon"><button aria-label="插入表情" data-tooltip="插入表情" data-tooltip-position="bottom" data-tooltip-will-hide-on-click="true" id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content" type="button" class="Button Editable-control Button--plain"><svg class="Zi Zi--Emotion" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M7.523 13.5h8.954c-.228 2.47-2.145 4-4.477 4-2.332 0-4.25-1.53-4.477-4zM12 21a9 9 0 1 1 0-18 9 9 0 0 1 0 18zm0-1.5a7.5 7.5 0 1 0 0-15 7.5 7.5 0 0 0 0 15zm-3-8a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm6 0a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3z"></path></svg></button></div></div></div></div><button type="button" disabled="" class="Button CommentEditorV2-singleButton Button--primary Button--blue">发布</button></div></div><div><div class="CommentListV2"><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover12-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover12-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/feng-heng-xin-15"><img class="Avatar UserLink-avatar" width="24" height="24" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-ad8b153078ac792965877833f8422953_s.jpg" srcset="https://pic4.zhimg.com/v2-ad8b153078ac792965877833f8422953_xs.jpg?source=06d4cd63 2x" alt="陈瑾"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/feng-heng-xin-15">陈瑾</a></span><span class="CommentItemV2-time">2020-12-17</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">这是我看过DTER解读最详细的文章，赞！</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>1</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover13-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover13-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xieqingtao"><img class="Avatar UserLink-avatar" width="24" height="24" src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/v2-8d2597784f4f6e279e3b5d0afa4c83d7_s.jpg" srcset="https://pic4.zhimg.com/v2-8d2597784f4f6e279e3b5d0afa4c83d7_xs.jpg?source=06d4cd63 2x" alt="青山不改"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/xieqingtao">青山不改</a></span><span class="CommentItemV2-roleInfo"> (作者) </span><span class="CommentItemV2-time">2020-12-17</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext">文中有些md文本没有转换好格式，后面会修改（吐槽下知乎的markdown编辑太差劲）</div></div><div class="CommentItemV2-footer"><button type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Reply" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M22.959 17.22c-1.686-3.552-5.128-8.062-11.636-8.65-.539-.053-1.376-.436-1.376-1.561V4.678c0-.521-.635-.915-1.116-.521L1.469 10.67a1.506 1.506 0 0 0-.1 2.08s6.99 6.818 7.443 7.114c.453.295 1.136.124 1.135-.501V17a1.525 1.525 0 0 1 1.532-1.466c1.186-.139 7.597-.077 10.33 2.396 0 0 .396.257.536.257.892 0 .614-.967.614-.967z" fill-rule="evenodd"></path></svg></span>回复</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="transform: rotate(180deg); margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>踩</button><button type="button" class="Button CommentItemV2-hoverBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Report" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M19.947 3.129c-.633.136-3.927.639-5.697.385-3.133-.45-4.776-2.54-9.949-.888-.997.413-1.277 1.038-1.277 2.019L3 20.808c0 .3.101.54.304.718a.97.97 0 0 0 .73.304c.275 0 .519-.102.73-.304.202-.179.304-.418.304-.718v-6.58c4.533-1.235 8.047.668 8.562.864 2.343.893 5.542.008 6.774-.657.397-.178.596-.474.596-.887V3.964c0-.599-.42-.972-1.053-.835z" fill-rule="evenodd"></path></svg></span>举报</button></div></div></div></div></li></ul></div></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex CornerAnimayedFlex--hidden"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" aria-label="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","fetchRoot":{"www":"https:\u002F\u002Fwww.zhihu.com","api":"https:\u002F\u002Fapi.zhihu.com","lens":"https:\u002F\u002Flens.zhihu.com","zhuanlan":"https:\u002F\u002Fzhuanlan.zhihu.com","walletpay":"https:\u002F\u002Fwalletpay.zhihu.com","captcha":"https:\u002F\u002Fcaptcha.zhihu.com"}}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"club":{"tags":{},"admins":{"data":[]},"members":{"data":[]},"explore":{"candidateSyncClubs":{}},"profile":{},"checkin":{},"comments":{"paging":{},"loading":{},"meta":{},"ids":{}},"postList":{"paging":{},"loading":{},"ids":{}},"recommend":{"data":[]},"silences":{"data":[]},"application":{"profile":null}},"entities":{"users":{"xieqingtao":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7.jpg?source=172ae18b","uid":"769817390966935552","userType":"people","isFollowing":false,"urlToken":"xieqingtao","id":"82d9972d4704613c3794f9c219fe376a","description":"若以写作为生，愿天涯为笔，海角为画。","name":"青山不改","isAdvertiser":false,"headline":"在CV苦海挣扎的研究僧","gender":1,"url":"\u002Fpeople\u002F82d9972d4704613c3794f9c219fe376a","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b","isOrg":false,"type":"people","levelInfo":{"exp":48528,"level":6,"nicknameColor":{"color":"","nightModeColor":""},"levelIcon":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","iconInfo":{"url":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","nightModeUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-92beb29b03851affbd2e8114805460cb_l.png","width":93,"height":51}},"badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"972464154808860672","medalName":"社交达人","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-86a22bb4de0d114c3dbdf71b071c0167_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-119b85aa41a4231a7dbfe85a46a94ddc_l.png?source=172ae18b","description":"关注 20 个人"}}},"questions":{},"answers":{},"articles":{"337649487":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=__MEMBERID__&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__&zid=__ZONEID__"],"id":337649487,"title":"End-to-End Object Detection with Transformers论文详解","type":"article","articleType":"normal","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F337649487","imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-99e7ec86dddff81164501e45b4590790_720w.jpg?source=172ae18b","titleImage":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-99e7ec86dddff81164501e45b4590790_720w.jpg?source=172ae18b","excerpt":"FAIR对DETR简单介绍\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fai.facebook.com\u002Fblog\u002Fend-to-end-object-detection-with-transformers\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eai.facebook.com\u002Fblog\u002Fen\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ed-to-end-object-detection-with-transformers\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E首先是一些总括，本文方法在文章中部。 1.舍弃一些手工部件，不依赖NMS和先验框，或者是Anchor的设定,简化了检测的pipeline2.DETR架构包括两部分：\u003Cb\u003E基于集合的全局损失与Transfomer encoder-decoder\u003C\u002Fb\u003E。其中DETR framew…","created":1608193719,"updated":1608256804,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7.jpg?source=172ae18b","uid":"769817390966935552","userType":"people","isFollowing":false,"urlToken":"xieqingtao","id":"82d9972d4704613c3794f9c219fe376a","description":"若以写作为生，愿天涯为笔，海角为画。","name":"青山不改","isAdvertiser":false,"headline":"在CV苦海挣扎的研究僧","gender":1,"url":"\u002Fpeople\u002F82d9972d4704613c3794f9c219fe376a","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b","isOrg":false,"type":"people","levelInfo":{"exp":48528,"level":6,"nicknameColor":{"color":"","nightModeColor":""},"levelIcon":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","iconInfo":{"url":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-4a3a25e3b2a871617ac0e3185a93dc14_l.png","nightModeUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-92beb29b03851affbd2e8114805460cb_l.png","width":93,"height":51}},"badge":[],"badgeV2":{"title":"","mergedBadges":[],"detailBadges":[],"icon":"","nightIcon":""},"exposedMedal":{"medalId":"972464154808860672","medalName":"社交达人","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-86a22bb4de0d114c3dbdf71b071c0167_r.png?source=172ae18b","miniAvatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-119b85aa41a4231a7dbfe85a46a94ddc_l.png?source=172ae18b","description":"关注 20 个人"}},"commentPermission":"all","copyrightPermission":"need_review","state":"published","imageWidth":1676,"imageHeight":942,"content":"\u003Cblockquote\u003EFAIR对DETR简单介绍\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fai.facebook.com\u002Fblog\u002Fend-to-end-object-detection-with-transformers\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002F\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Eai.facebook.com\u002Fblog\u002Fen\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003Ed-to-end-object-detection-with-transformers\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E\u003Cp\u003E首先是一些总括，本文方法在文章中部。\u003C\u002Fp\u003E\u003Ch2\u003E1.舍弃一些手工部件，不依赖NMS和先验框，或者是Anchor的设定,简化了检测的pipeline\u003C\u002Fh2\u003E\u003Ch2\u003E2.DETR架构包括两部分：\u003Cb\u003E基于集合的全局损失与Transfomer encoder-decoder\u003C\u002Fb\u003E。\u003C\u002Fh2\u003E\u003Cp\u003E其中DETR framework包含一基于\u003Cb\u003E集合的全局损失\u003C\u002Fb\u003E，通过\u003Cb\u003E二分图匹配\u003C\u002Fb\u003E来强制\u003Cb\u003E唯一的预测\u003C\u002Fb\u003E，以前也有RNN对目标检测的尝试，但是速度太慢，无法并行计算。\u003Cb\u003ETransfomer可并行计算。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ch2\u003E3.  DETR的工作基于这两个方面：\u003Cb\u003Eset predication、Transformer\u003C\u002Fb\u003E\u003C\u002Fh2\u003E\u003Ch3\u003E3.1     集合预测的二分图匹配 \u003Cb\u003Ebipartite matching loss for set prediction\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp\u003E传统方法为了避免相近框的重叠问题，选择使用后处理的方法，如非极大 值抑制，\u003C\u002Fp\u003E\u003Cp\u003E但是\u003Cb\u003Eset prediction是无后处理的\u003C\u002Fb\u003E。他们需要对所有预测元素之间的交互进行建模的全局推理方案，以避免冗余。\u003C\u002Fp\u003E\u003Cp\u003E对于恒定的集合预测，可以使用\u003Cb\u003E密集的全连接层\u003C\u002Fb\u003E，有效但是\u003Cb\u003E耗费太多算力\u003C\u002Fb\u003E,对于set prediction 这种问题一般的方法使用&#39;auto-regressive sequence models&#39;，例如RNN。\u003C\u002Fp\u003E\u003Cp\u003E损失函数在经过prediction的置换之后应该不变，通用的解决方法是设计一个基于\u003Cb\u003E匈牙利算法\u003C\u002Fb\u003E的损失函数，找到GT与预测的最优匹配。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E匈牙利算法\u003C\u002Fb\u003E就是一个最优匹配问题，从矩阵角度来说，就是每一行抽出最小值，然后让该行所有值都去减掉抽出的最小值，会至少获得一个0，然后剩余的所有行也做这样的操作。\u003C\u002Fp\u003E\u003Cp\u003E第一次全部操作完后，如果每一个行都有一个不同列的0值，那么此时就是各个0值位置最佳的匹配。\u003C\u002Fp\u003E\u003Cp\u003E如果0的行列都不相同，则不必再计算，否则，继续上述操作，直到0值的行列均不相同。\u003C\u002Fp\u003E\u003Ch3\u003E3.2  \u003Cb\u003Eencoder-decoder architectures based on the Transfomer\u003C\u002Fb\u003E\u003C\u002Fh3\u003E\u003Cp\u003E基于注意力机制模型的一个主要优点是他能够进行全局的并行计算，和优越的记忆能力。\u003C\u002Fp\u003E\u003Cp\u003E因此在long sequences上比RNN更合适。\u003C\u002Fp\u003E\u003Ch2\u003E4. OD相关工作。\u003C\u002Fh2\u003E\u003Cp\u003E\u003Cb\u003ETwo-stage predict boxs w.r.t. proposals\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003Eone-stage predict anchor or a grid of possible object centers\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EAnchor-base 与 anchor-free\u003C\u002Fb\u003E  Bridging the Gap Between Anchor-based and Anchor-free\u003C\u002Fp\u003E\u003Cp\u003EDetection via Adaptive Training Sample Selection 研究发现两者区别在于正负样本的选取方法不同。\u003Cb\u003E论文提出ATSS\u003C\u002Fb\u003E。**本文则与两种方法都不同，舍弃这种预先设置，直接用absolute box预测输入图片，而非预测anchor。\u003C\u002Fp\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003Eset-based loss\u003C\u002Fb\u003E 早期采用二分匹配损失，如SSD等算法(见论文书签object detection)，然而不同预测之间的关系用CNN或这全连接层来表示，并用手工设计的后处理来提高性能。\u003Cbr\u002F\u003E而最近的一些算法，如Faster RCNN，focal loss，在GT与预测间用非唯一分配的规则，并且用上NMS。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003Elearnable NMS\u003C\u002Fb\u003E可学习的NMS等方法，对不同预测间的关系用attention进行建模。使用direct set损失，它们不需要任何后处理步骤。但是，这些方法采用了其他手工制作的上下文特征（例如提案框坐标）来有效地对检测之间的关系进行建模，而我们正在寻找能够减少模型中编码的先验知识的解决方案。\u003C\u002Fp\u003E\u003Ch2\u003E5.模型可以查看图像的其他区域，以帮助对边界框中的对象作出决定。\u003C\u002Fh2\u003E\u003Cp\u003E它还可以\u003Cb\u003E根据图像中对象之间的关系或相关性进行预测\u003C\u002Fb\u003E。 例如，如果DETR预测图像包含一个站在海滩上的人，它就知道部分被遮挡的物体更有可能是冲浪板。 相反，其他检测模型只是机械的地预测每个对象。\u003C\u002Fp\u003E\u003Ch2\u003E6.促进视觉与NLP领域的结合，对以后\u003Cb\u003E同时处理视觉与语言信息\u003C\u002Fb\u003E提供一定的思路\u003C\u002Fh2\u003E\u003Chr\u002F\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Ch2\u003E本文方法 The DETR model(位于论文第三节)\u003C\u002Fh2\u003E\u003Ch2\u003E3.1.set prediction loss\u003C\u002Fh2\u003E\u003Cp\u003E一次的预测N个，N大于一张图片中的目标个数，这里的一个主要困难是如何对GT与预测之间的类别，位置，\u003C\u002Fp\u003E\u003Cp\u003E大小进行评分比对。损失函数会产生一个最佳的二分匹配，然后对包围框进行特定优化。\u003Cbr\u002F\u003E 关于二分图匹配，其实就是一个最优匹配问题。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7B%5Csigma%7D%3D+argmin%5Csum_%7Bi%7D%5E%7BN%7D%28L_%7Bmatch%7D%28y_%7Bi%7D%2C%5Chat%7By%7D_%7B%5Csigma+%28i%29%7D%29++++++++++++++++%EF%BC%881%EF%BC%89\" alt=\"\\hat{\\sigma}= argmin\\sum_{i}^{N}(L_{match}(y_{i},\\hat{y}_{\\sigma (i)})                （1）\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E  上式中，N表示一次预测的个数，大于图片中的目标个数，y表示预测值，并且y也是N个，不足的用空集（NO-object）来pading。y_hat 表示预测的N个。为了匹配这两个集合（y和y_hat），用上面这个式子来计算。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L_%7Bmatch%7D%28y_%7Bi%7D%2C%5Chat%7By%7D_%7B%5Csigma+%28i%29%7D%29\" alt=\"L_{match}(y_{i},\\hat{y}_{\\sigma (i)})\" eeimg=\"1\"\u002F\u003E 是成对匹配的cost，用匈牙利算法来计算。\u003C\u002Fp\u003E\u003Cp\u003E  匹配cost考虑到了类别的损失，和边界框的损失。\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=y_%7Bi%7D+%3D+%28c_%7Bi%7D%2Cb_%7Bi%7D%29\" alt=\"y_{i} = (c_{i},b_{i})\" eeimg=\"1\"\u002F\u003E,其中\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_%7Bi%7D\" alt=\"c_{i}\" eeimg=\"1\"\u002F\u003E 表示的是类别标签，可能为空集。\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=b_%7Bi%7D+%3D+%5B0%2C1%5D%5E%7B4+%7D\" alt=\"b_{i} = [0,1]^{4 }\" eeimg=\"1\"\u002F\u003E 表示的是box的中心坐标，和高和宽相对于image size（感觉第四个是image size）。\u003C\u002Fp\u003E\u003Cp\u003E对于 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Csigma_%7Bi%7D\" alt=\"\\sigma_{i}\" eeimg=\"1\"\u002F\u003E 定义了  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_%7Bi%7D\" alt=\"c_{i}\" eeimg=\"1\"\u002F\u003E 其具体为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7Bp%7D_%7B%5Csigma%28i%29%7D%28C_%7Bi%7D%29\" alt=\"\\hat{p}_{\\sigma(i)}(C_{i})\" eeimg=\"1\"\u002F\u003E ，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7Bb%7D_%7B%5Csigma+%28i%29%7D\" alt=\"\\hat{b}_{\\sigma (i)}\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E预测box,\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L_%7Bmatch%7D%28y_%7Bi%7D%2C%5Chat%7By%7D_%7B%5Csigma+%28i%29%7D%29\" alt=\"L_{match}(y_{i},\\hat{y}_{\\sigma (i)})\" eeimg=\"1\"\u002F\u003E 具体为\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=-1_%7B%5C%7BC_%7Bi%7D%5Cneq%5Cphi%5C%7D%7D%5Chat%7BP%7D_%7B%5Csigma%28i%29%7D%28c_%7Bi%7D%29%2B1_%7B%5C%7BC_%7Bi%7D%5Cneq%5Cphi%5C%7D%7DL_%7Bbox%7D%28b_%7Bi%7D%2C%5Chat%7Bb%7D_%7B%5Csigma%28i%29%7D%29\" alt=\"-1_{\\{C_{i}\\neq\\phi\\}}\\hat{P}_{\\sigma(i)}(c_{i})+1_{\\{C_{i}\\neq\\phi\\}}L_{box}(b_{i},\\hat{b}_{\\sigma(i)})\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E这种寻找匹配的过程与anchor和proposals与GT匹配的过程有相同的作用，最大的区别是找到一对一的匹配进行\u003Cb\u003E无重复的直接 set predict\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E损失函数\u003C\u002Fb\u003E，就是类别损失与box损失的线性组合:\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L_%7BHungarian%7D%28y%2C%5Chat%7By%7D%29+%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%5B-%5Clog%5Chat%7BP%7D_%7B%5Csigma%28i%29%7D%28C_%7Bi%7D%29%2B1_%7B%5C%7BC_%7Bi%7D%5Cneq%5Cphi%5C%7D%7DL_%7Bbox%7D%28b_%7Bi%7D%2C%5Chat%7Bb%7D_%7B%5Csigma%28i%29%7D%29%5D++++++++++++++++%EF%BC%882%EF%BC%89\" alt=\"L_{Hungarian}(y,\\hat{y}) =\\sum_{i=1}^{N}[-\\log\\hat{P}_{\\sigma(i)}(C_{i})+1_{\\{C_{i}\\neq\\phi\\}}L_{box}(b_{i},\\hat{b}_{\\sigma(i)})]                （2）\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E（2）式中，\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7B%5Csigma%7D\" alt=\"\\hat{\\sigma}\" eeimg=\"1\"\u002F\u003E是（1）中的最佳匹配，当\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=c_%7Bi%7D%3D%5Cphi\" alt=\"c_{i}=\\phi\" eeimg=\"1\"\u002F\u003E时，把log-probability 权重降低10倍，来解决正负样本不平衡，这类似于Faster R-CNN中，训练过程为了平衡正负样本所采取的subsampling（二次抽样）。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E后面说时在目标和\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"\u002F\u003E之间的匹配cost不依赖于预测，这意味着这种情况下，cost是固定的，因此在匹配cost里，使用了 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Chat%7BP%7D_%7B%5Csigma%28i%29%7D%28c_%7Bi%7D%29\" alt=\"\\hat{P}_{\\sigma(i)}(c_{i})\" eeimg=\"1\"\u002F\u003E来代替log-probability .然后这就i让类预测能够通约到\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L_%7Bbox%28.%2C.%29%7D\" alt=\"L_{box(.,.)}\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fb\u003E不同于其他方法那样有先验框去预测周边误差，我们的方法是直接预测框，这种方法虽然更直接，但是带来的问题是损失的相对尺度问题。直接使用L1损失对不同大小box会有不同尺度的问题，\u003Cb\u003E为了缓解这个问题\u003C\u002Fb\u003E 使用L1损失和广义IOU损失的线性组合，该尺度是不变的。（为啥不变呢？）\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=L_%7Bbox%7D%28b_%7Bi%7D%2C%5Chat%7Bb%7D_%7B%5Csigma%28i%29%7D%29\" alt=\"L_{box}(b_{i},\\hat{b}_{\\sigma(i)})\" eeimg=\"1\"\u002F\u003E 具体表示为：\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda_%7Biou%7DL_%7Biou%7D%28b_%7Bi%7D%2C%5Chat%7Bb%7D_%7B%5Csigma%28i%29%7D%29%2B%5Clambda_%7BL1%7D%7C%7Cb_%7Bi%7D-%5Chat%7Bb%7D_%7B%5Csigma%28i%29%7D%7C%7C_%7B1%7D\" alt=\"\\lambda_{iou}L_{iou}(b_{i},\\hat{b}_{\\sigma(i)})+\\lambda_{L1}||b_{i}-\\hat{b}_{\\sigma(i)}||_{1}\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E 其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda_%7Biou%7D%2C%5Clambda_%7BL1%7D\" alt=\"\\lambda_{iou},\\lambda_{L1}\" eeimg=\"1\"\u002F\u003E 都属于R，都是超参数。\u003C\u002Fp\u003E\u003Cp\u003E这两个损失函数都在batch中用目标数目来进行归一化。\u003C\u002Fp\u003E\u003Ch2\u003E3.2 DETR的架构\u003C\u002Fh2\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16dfa1fb8d1c6d8048caf760f92958f3_b.png\" data-size=\"normal\" class=\"content_image\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-16dfa1fb8d1c6d8048caf760f92958f3_b.png\"\u002F\u003E\u003Cfigcaption\u003EImage\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-10eabf880fe88dc59c9673952e6cb5fb_b.png\" data-size=\"normal\" class=\"content_image\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-10eabf880fe88dc59c9673952e6cb5fb_b.png\"\u002F\u003E\u003Cfigcaption\u003EImage\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E如图所示，DETR的架构很简单（官方简单。。）包含三个部分：\u003Cb\u003E1.CNN主干来提取特征\u003C\u002Fb\u003E，\u003Cb\u003E2.encoder-decoder Transfomer,3.FFN，完成最后的检测预测。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E实现相对简单，只要能提供CNN和Transfomer就能够在任何深度框架下用50行代码实现（pytorch）\u003C\u002Fp\u003E\u003Ch3\u003E1.Backbone（骨干网络）\u003C\u002Fh3\u003E\u003Cp\u003E使用CNN提取特征，把\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=3%5Ctimes+H%5Ctimes+W\" alt=\"3\\times H\\times W\" eeimg=\"1\"\u002F\u003E的图片变成  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=2048%5Ctimes+%5Cfrac%7BH%7D%7B32%7D%5Ctimes+%5Cfrac%7BW%7D%7B32%7D\" alt=\"2048\\times \\frac{H}{32}\\times \\frac{W}{32}\" eeimg=\"1\"\u002F\u003E的特征图，这里要注意，对于同一batch的图片，要对其resize，\u003Cb\u003E用0-padding方法，保证他们的(H,W)相同\u003C\u002Fb\u003E。\u003C\u002Fp\u003E\u003Ch3\u003E2.Transfomer encoder（具体其实可看论文Attention is all  you need）\u003C\u002Fh3\u003E\u003Cp class=\"ztext-empty-paragraph\"\u003E\u003Cbr\u002F\u003E\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d1f2e4794f41020128e56b3f10a9108f_b.png\" data-size=\"normal\" class=\"content_image\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;\u002Fsvg&gt;\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-d1f2e4794f41020128e56b3f10a9108f_b.png\"\u002F\u003E\u003Cfigcaption\u003EImage\u003C\u002Ffigcaption\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E\u003Cbr\u002F\u003E2.1 首先用 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=1%5Ctimes+1\" alt=\"1\\times 1\" eeimg=\"1\"\u002F\u003E的卷积讲原来C=2048维降到d维，就形成了一个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d+%5Ctimes+H%5Ctimes+W\" alt=\"d \\times H\\times W\" eeimg=\"1\"\u002F\u003E 的特征图，encoder的输入必须是序列，因此再变成 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d%5Ctimes+HW\" alt=\"d\\times HW\" eeimg=\"1\"\u002F\u003E 的特征图，这里每一个encoder都有\u003Cb\u003Emulti-head self-attention和FFN\u003C\u002Fb\u003E.因为transformer架构的原因，其缺少位置信息（\u003Cb\u003Epermutation-invariant\u003C\u002Fb\u003E），还需为每个attention层加入位置信息（不是学到的，是手工的，学到的效果不好）\u003C\u002Fp\u003E\u003Cp\u003E这里 \u003Cb\u003Epermutation-invariant\u003C\u002Fb\u003E指的是特征之间没有空间位置关系，例如多层感知机，改变像素的位置对最后的结果没有影响，但是对于\u003Cb\u003E卷积神经网络\u003C\u002Fb\u003E而言，特征是有位置关系的。这里对于\u003Cb\u003ETransfomer架构来说，是没有位置关系的\u003C\u002Fb\u003E因此需要添加位置信息。\u003C\u002Fp\u003E\u003Ch3\u003E3.Transfomer decoder\u003C\u002Fh3\u003E\u003Cp\u003ETransfomer decoder 由\u003Cb\u003E多头自注意力和encoder-decoder注意力机制\u003C\u002Fb\u003E组成。与传统的Trans不同的地方在于对N个objects进行decoder时是\u003Cb\u003E并行\u003C\u002Fb\u003E的。\u003C\u002Fp\u003E\u003Cblockquote\u003E这里解码时候，也是permutation-invariant的\u003C\u002Fblockquote\u003E\u003Cp\u003E因此N个input特征必须是不同的来产生不同的结果，这些输入的特征是学到的位置编码信息，称之为\u003Ci\u003E\u003Cb\u003Eobject queriers\u003C\u002Fb\u003E\u003C\u002Fi\u003E.同样将这些位置编码输入到所有的attention层。解码器将这N个对象查询转换成embeding的输出。然后，它们被前馈网络(在下一小节中描述)独立解码成box坐标和类别标签，得到N个最终预测。利用self- and encoder-decoder attention，该模型利用所有对象之间的成对关系对所有对象进行全局推理，同时能够将整个图像用作上下文。\u003C\u002Fp\u003E\u003Ch3\u003E4.Prediction feed-forward networks(FFNs)（预测前馈网络）\u003C\u002Fh3\u003E\u003Cp\u003E最终的预测是由一个\u003Cb\u003E3层感知器\u003C\u002Fb\u003E和一个\u003Cb\u003E线性投影层\u003C\u002Fb\u003E计算的，感知器具有ReLU激活函数和隐藏维数d。FFN预测输入图像中盒子的归一化中心坐标、高度和宽度，线性层使用softmax函数预测类别标签。因为我们预测一组固定大小的n个边界框，其中n通常比图像中感兴趣对象的实际数量大得多，所以使用额外的特殊类标签∅来表示在一个槽内没有检测到对象。该类在标准对象检测方法中扮演着与“背景”类相似的角色。\u003C\u002Fp\u003E\u003Cblockquote\u003Elinear projection layer（\u003Cb\u003E线性投影层\u003C\u002Fb\u003E）：Projection layer是为了减少计算量，它的作用和全连接layer很像，就是对输出向量做一下压缩，从而能把高纬度的信息降维，减小cell unit的维度，从而减小相关参数矩阵的参数数目。  projection layer也是使用 1x1Conv 的网络结构，作用是希望把高维特征映射到低维空间去。\u003C\u002Fblockquote\u003E\u003Ch3\u003E5.Auxiliary decoding losses(辅助解码损失).\u003C\u002Fh3\u003E\u003Cp\u003E我们发现在训练期间在解码器中使用\u003Ci\u003E\u003Cb\u003Eauxiliary losses\u003C\u002Fb\u003E\u003C\u002Fi\u003E是有帮助的，特别是有助于模型输出每个类的正确对象数。我们在每个解码器层后添加预测FFNs和匈牙利损失。所有预测FFNs共享它们的参数。我们使用一个额外的共享层范数来标准化来自不同解码器层的预测帧速率网络的输入。\u003C\u002Fp\u003E\u003Cblockquote\u003Eauxiliary losses：这种方法是  《 Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character-level language modeling with deeper self-attention. In: AAAI Conference on Artificial Intelligence (2019)》这篇论文提到的， 大概就是辅助损失有助于优化学习过程。\u003C\u002Fblockquote\u003E\u003Ch2\u003E实验\u003C\u002Fh2\u003E\u003Cp\u003E···后面补充\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19596960","type":"topic","id":"19596960","name":"目标检测"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19590195","type":"topic","id":"19590195","name":"计算机视觉"}],"voteupCount":8,"voting":0,"column":{"description":"","canManage":false,"intro":"小样本的目标检测领域，及机器视觉和深度学习相关","isFollowing":false,"urlToken":"c_1312325735072657408","id":"c_1312325735072657408","articlesCount":4,"acceptSubmission":true,"title":"计算机视觉","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1312325735072657408","commentPermission":"all","created":1605662837,"updated":1608256994,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7.jpg?source=172ae18b","uid":"769817390966935552","userType":"people","isFollowing":false,"urlToken":"xieqingtao","id":"82d9972d4704613c3794f9c219fe376a","description":"若以写作为生，愿天涯为笔，海角为画。","name":"青山不改","isAdvertiser":false,"headline":"在CV苦海挣扎的研究僧","gender":1,"url":"\u002Fpeople\u002F82d9972d4704613c3794f9c219fe376a","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":0,"type":"column"},"commentCount":2,"contributions":[{"id":27979052,"state":"accepted","type":"first_publish","column":{"description":"","canManage":false,"intro":"小样本的目标检测领域，及机器视觉和深度学习相关","isFollowing":false,"urlToken":"c_1312325735072657408","id":"c_1312325735072657408","articlesCount":4,"acceptSubmission":true,"title":"计算机视觉","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1312325735072657408","commentPermission":"all","created":1605662837,"updated":1608256994,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7.jpg?source=172ae18b","uid":"769817390966935552","userType":"people","isFollowing":false,"urlToken":"xieqingtao","id":"82d9972d4704613c3794f9c219fe376a","description":"若以写作为生，愿天涯为笔，海角为画。","name":"青山不改","isAdvertiser":false,"headline":"在CV苦海挣扎的研究僧","gender":1,"url":"\u002Fpeople\u002F82d9972d4704613c3794f9c219fe376a","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":0,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"favlistsCount":11,"isNormal":true,"status":0,"shareText":"End-to-End Object Detection with Transformers论文详解 - 来自知乎专栏「计算机视觉」，作者: 青山不改 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F337649487 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1,"isVisible":true,"isLiked":false,"likedCount":0,"visibleOnlyToAuthor":false,"hasColumn":true,"republishers":[]}},"columns":{"c_1312325735072657408":{"description":"","canManage":false,"intro":"小样本的目标检测领域，及机器视觉和深度学习相关","isFollowing":false,"urlToken":"c_1312325735072657408","id":"c_1312325735072657408","articlesCount":4,"acceptSubmission":true,"title":"计算机视觉","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fc_1312325735072657408","commentPermission":"all","created":1605662837,"updated":1608256994,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F4b70deef7_720w.jpg?source=172ae18b","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7.jpg?source=172ae18b","uid":"769817390966935552","userType":"people","isFollowing":false,"urlToken":"xieqingtao","id":"82d9972d4704613c3794f9c219fe376a","description":"若以写作为生，愿天涯为笔，海角为画。","name":"青山不改","isAdvertiser":false,"headline":"在CV苦海挣扎的研究僧","gender":1,"url":"\u002Fpeople\u002F82d9972d4704613c3794f9c219fe376a","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-8d2597784f4f6e279e3b5d0afa4c83d7_l.jpg?source=172ae18b","isOrg":false,"type":"people"},"followers":0,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"zvideosByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{},"creationsFeed":{},"infinity":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-qa_cl_guest-2","expPrefix":"qa_cl_guest","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_item-3","expPrefix":"se_item","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_andplay_d-2","expPrefix":"vd_andplay_d","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_hookupplay_an-2","expPrefix":"vd_hookupplay_an","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_timeguide-2","expPrefix":"vd_timeguide","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_video_replay-3","expPrefix":"vd_video_replay","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_link-10","expPrefix":"vd_zvideo_link","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"zanswer-2","expPrefix":"zanswer","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"recnew_2th-3","expPrefix":"recnew_2th","isDynamicallyUpdated":false,"isRuntime":false,"includeTriggerInfo":false},{"expId":"vd_bullet_gui-2","expPrefix":"vd_bullet_gui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"captcha_v2-1_v9","expPrefix":"captcha_v2","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"Test_Punk-1_v2","expPrefix":"Test_Punk","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"meta_ebook-2_v1","expPrefix":"meta_ebook","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"club_fn-1_v4","expPrefix":"club_fn","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"webpImg-1_v3","expPrefix":"webpImg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"general_1-2_v1","expPrefix":"general_1","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"correct_gpu-6_v5","expPrefix":"correct_gpu","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"correct_pos-4_v2","expPrefix":"correct_pos","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_cvr_boost-3_v1","expPrefix":"se_cvr_boost","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_doc_cnt-1_v6","expPrefix":"se_doc_cnt","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_new2th-2_v3","expPrefix":"rec_new2th","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_zp_five-4_v1","expPrefix":"se_zp_five","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"hw_aa_30-1_v2","expPrefix":"hw_aa_30","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"hw_aa_50-1_v1","expPrefix":"hw_aa_50","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_4a","type":"Int","value":"0","chainId":"_gene_","layerId":"rtiq","key":335},{"id":"gue_recmess","type":"String","value":"0","layerId":"gueqa_layer_795"},{"id":"ge_guess","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_938","key":2912},{"id":"gue_andplayd","type":"String","value":"1","layerId":"guevd_layer_686"},{"id":"se_doc_cnt","type":"Int","value":"0","chainId":"_gene_","layerId":"uD1x","key":302},{"id":"web_scl_rec","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"ge_video","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":2831},{"id":"gue_cdzixun","type":"String","value":"0","layerId":"gueqa_layer_3"},{"id":"gue_repost","type":"String","value":"0","layerId":"gueqa_layer_671"},{"id":"ge_v_rank_v3","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1047","key":2966},{"id":"pf_adjust","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_9"},{"id":"hw_aa_50","type":"Int","value":"0","chainId":"_gene_","layerId":"hw_aa_50","key":362},{"id":"gue_art_ui","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"gue_fo_recom","type":"String","value":"0","layerId":"gueqa_layer_780"},{"id":"ge_sxzx","type":"String","value":"0","chainId":"_gene_","layerId":"gere_layer_990","key":3060},{"id":"gue_bulletmb","type":"String","value":"0","layerId":"guevd_layer_812"},{"id":"pfd_newbie","type":"Int","value":"0","chainId":"_gene_","layerId":"pfd_newbie","key":63},{"id":"li_sp_mqbk","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_748"},{"id":"zr_slotpaidexp","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_5"},{"id":"tp_dingyue_video","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"ge_prf_rec","type":"String","value":"0","chainId":"_gene_","layerId":"getop_layer_991","key":3040},{"id":"gue_vid_tab","type":"String","value":"0","layerId":"guevd_layer_900"},{"id":"tp_zrec","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_619"},{"id":"Test_Punk","type":"Int","value":"0","layerId":"Test_Punk"},{"id":"ge_rec_sup","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_rec_sup","key":197},{"id":"se_prerank","type":"Int","value":"0","chainId":"_gene_","layerId":"6XUc","key":354},{"id":"gue_card_test","type":"String","value":"1","layerId":"gueqa_layer_2"},{"id":"ge_dipin_pre","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3124},{"id":"ge_sug_v2","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_1000","key":3189},{"id":"ge_emoji","type":"String","value":"0","chainId":"_gene_","layerId":"getp_layer_827","key":3209},{"id":"gue_iosplay","type":"String","value":"0","layerId":"guevd_layer_896"},{"id":"rec_new2th","type":"Int","value":"1","chainId":"_gene_","layerId":"Hump","key":320},{"id":"gue_art2qa","type":"String","value":"0","layerId":"gueqa_layer_579"},{"id":"web_answerlist_ad","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"web_audit_01","type":"String","value":"case1","layerId":"webre_layer_1"},{"id":"ge_newyanzhi","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_1019","key":2788},{"id":"gue_bullet_guide","type":"String","value":"分享你刚编的弹幕","layerId":"guevd_layer_0"},{"id":"gue_v_serial","type":"String","value":"1","layerId":"guevd_layer_695"},{"id":"li_edu_page","type":"String","value":"old","chainId":"_all_","layerId":"lili_layer_580"},{"id":"gue_goods_card","type":"String","value":"0","layerId":"gueqa_layer_1"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"web_answer_list_ad","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"ge_meta_ss","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_834","key":3079},{"id":"tp_contents","type":"String","value":"2","chainId":"_all_","layerId":"tptp_layer_627"},{"id":"gue_video_guide","type":"String","value":"1","layerId":"guevd_layer_625"},{"id":"correct_gpu","type":"Int","value":"5","chainId":"_gene_","layerId":"correct_gpu","key":66},{"id":"ge_rec_2th","type":"String","value":"11","chainId":"_gene_","layerId":"geli_layer_965","key":3023},{"id":"ge_relation2","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_815","key":2796},{"id":"ge_item","type":"String","value":"2","chainId":"_gene_","layerId":"gese_layer_945","key":2971},{"id":"ge_upload","type":"String","value":"0","chainId":"_gene_","layerId":"geus_layer_839","key":2892},{"id":"zanswer","type":"Int","value":"1","layerId":"zanswer"},{"id":"webpImg","type":"Int","value":"1","layerId":"JnVt"},{"id":"se_fix_ebook","type":"Int","value":"0","chainId":"_gene_","layerId":"se_fix_ebook","key":103},{"id":"li_panswer_topic","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_602"},{"id":"web_column_auto_invite","type":"String","value":"0","layerId":"webqa_layer_1"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_","layerId":"pfus_layer_718"},{"id":"correct_pos","type":"Int","value":"2","chainId":"_gene_","layerId":"correct_pos","key":104},{"id":"web_sem_ab","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"gue_playh_an","type":"String","value":"1","layerId":"guevd_layer_622"},{"id":"captcha_v2","type":"Int","value":"0","layerId":"captcha_v2"},{"id":"hot_card","type":"Int","value":"0","chainId":"_gene_","layerId":"hot_card","key":108},{"id":"hw_aa_30","type":"Int","value":"0","chainId":"_gene_","layerId":"hw_aa_30","key":361},{"id":"web_login","type":"String","value":"0","layerId":"webgw_layer_759"},{"id":"ge_entity","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_946","key":3036},{"id":"se_zp_five","type":"Int","value":"1","chainId":"_gene_","layerId":"se_zp_five","key":344},{"id":"li_vip_verti_search","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_2"},{"id":"zr_expslotpaid","type":"String","value":"1","chainId":"_all_","layerId":"zrrec_layer_11"},{"id":"ge_yuzhi_v1","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1029","key":3127},{"id":"gue_profile_video","type":"String","value":"1","layerId":"guevd_layer_5"},{"id":"pfd_newbie2","type":"Int","value":"0","chainId":"_gene_","layerId":"pfd_newbie2","key":71},{"id":"gue_q_share","type":"String","value":"0","layerId":"gueqa_layer_647"},{"id":"web_collection_guest","type":"String","value":"1","layerId":"webqa_layer_4"},{"id":"ge_sug_rep","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_1034","key":3158},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_","layerId":"iosus_layer_1"},{"id":"club_fn","type":"Int","value":"1","layerId":"club_fn"},{"id":"se_cvr_boost","type":"Int","value":"1","chainId":"_gene_","layerId":"se_cvr_boost","key":183},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_7"},{"id":"gue_visit_n_artcard","type":"String","value":"1","layerId":"gueqa_layer_579"},{"id":"ge_infinity6","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_815","key":2817},{"id":"gue_zvideo_link","type":"String","value":"1","layerId":"guevd_layer_2"},{"id":"meta_ebook","type":"Int","value":"1","layerId":"meta_ebook"},{"id":"show_ad","type":"Int","value":"0","chainId":"_gene_","layerId":"show_ad","key":27},{"id":"ge_search_ui","type":"String","value":"1","chainId":"_gene_","layerId":"gese_layer_838","key":2898},{"id":"se_ffzx_jushen1","type":"String","value":"0","chainId":"_all_","layerId":"sese_layer_4"},{"id":"tp_topic_style","type":"String","value":"0","chainId":"_all_","layerId":"tptp_layer_4"},{"id":"ge_newcard","type":"String","value":"3","chainId":"_gene_","layerId":"geus_layer_839","key":2997},{"id":"gue_bullet_second","type":"String","value":"1","layerId":"guevd_layer_1"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_","layerId":"qapqa_layer_2"},{"id":"general_1","type":"Int","value":"2","chainId":"_gene_","layerId":"general_1","key":8},{"id":"recnew_2th","type":"Int","value":"21","chainId":"_gene_","layerId":"recnew_2th","key":67},{"id":"ge_newbie3","type":"Int","value":"0","chainId":"_gene_","layerId":"ge_newbie3","key":180},{"id":"web_heifetz_grow_ad","type":"String","value":"1","layerId":"webgw_layer_3"},{"id":"ge_hard_s_ma","type":"String","value":"0","chainId":"_gene_","layerId":"geli_layer_856","key":3031},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_","layerId":"lili_layer_3"},{"id":"gue_messrec","type":"String","value":"0","layerId":"gueqa_layer_769"},{"id":"ge_usercard1","type":"String","value":"0","chainId":"_gene_","layerId":"gese_layer_742","key":2740},{"id":"gue_video_replay","type":"String","value":"2","layerId":"guevd_layer_3"},{"id":"gue_sharp","type":"String","value":"1","layerId":"guevd_layer_686"}],"chains":[{"chainId":"_all_"}],"encodedParams":"ClJPAWALLgEPC5YLagH0Cz8A4AvFAGIBNAx1DIkMQAHkCgcMQgDPC+wKmwtMC2cAaABsAGkB3AtYATcMRwBWDLcAAQsbAFILtQsIAEMAtADXC7QKEikAAAAAAAAAAAAAAAAAAAEAAAULAQIAAAIAAAABAQABAQAAAQMCFQAAAA=="},"triggers":{}},"userAgent":{"Edge":false,"IE":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"AndroidDaily":false,"iOSDaily":false,"WxMiniProgram":false,"BaiduMiniProgram":false,"QQMiniProgram":false,"JDMiniProgram":false,"isWebView":false,"isMiniProgram":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; Win64; x64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F88.0.4324.104 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F337649487","query":{},"href":"http:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F337649487","host":"zhuanlan.zhihu.com"},"trafficSource":"production","edition":{"beijing":false,"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false,"baiduSearch":false,"googleSearch":false,"miniProgram":false,"xiaomi":false},"theme":"light","enableShortcut":true,"referer":"","xUDId":"ALBW7lSx3RGPTi9R9p7jWAJgr7IYbZrVm6w=","mode":"ssr","conf":{},"xTrafficFreeOrigin":"","ipInfo":{"cityName":"台中市","countryName":"中国","regionName":"台湾","countryCode":"TW"},"logged":false,"vars":{"passThroughHeaders":{}}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"mcn":{},"applyStatus":{},"videoSupport":{},"mcnManage":{},"tasks":{},"recentlyCreated":[]},"answers":{"voters":{},"copyrightApplicants":{},"favlists":{},"newAnswer":{},"concernedUpvoters":{},"simpleConcernedUpvoters":{},"paidContent":{},"settings":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"room":{"meta":{},"isFetching":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]},"hotDaily":{"data":[],"paging":{}},"hotHighlight":{"isFetching":false,"isDrained":false,"data":[],"paging":{}}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"c_1312325735072657408",null]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0,"articleDisclaimers":[]},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"columnCollection":{},"userProfit":{"permission":{"permissionStatus":{"zhiZixuan":0,"recommend":-1,"task":0,"plugin":0},"visible":false}},"mcn":{"bindInfo":{},"memberCategoryList":[],"producerList":[],"categoryList":[],"lists":{},"banners":{},"protocolStatus":{"isAgreedNew":true,"isAgreedOld":true},"probationCountdownDays":0},"zvideos":{"campaigns":{},"tagoreCategory":[],"recommendations":{},"insertable":{},"recruit":{"form":{"platform":"","nickname":"","followerCount":"","domain":"","contact":""},"submited":false,"ranking":[]},"club":{}},"republish":{}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/vendor.8d94b1270b86debd0e1c.js.下載"></script><script src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/column.app.fdf357105e188d9054d7.js.下載"></script><script src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/hm.js.下載" async=""></script><script src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/zap.js.下載"></script><script src="./End-to-End Object Detection with Transformers论文详解 - 知乎_files/push.js.下載"></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><div><div><div class="css-8pdeid"></div></div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete11-0" id="Popover10-toggle" aria-haspopup="true" aria-owns="Popover10-content" class="Input" placeholder="选择语言" value=""><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></label></div></div></div></div></div></body></html>